# ==============================================================================
# Agent-AI — Local Development (macOS with Herd)
# ==============================================================================

APP_NAME="Agent AI"
APP_ENV=local
APP_DEBUG=true
APP_URL="http://localhost:8080"
APP_TIMEZONE="Europe/Amsterdam"

# PostgreSQL (Herd)
DB_CONNECTION=pgsql
DB_HOST=postgres
DB_PORT=5432
DB_DATABASE=agent_ai
DB_USERNAME=agent_user
DB_PASSWORD=

# Redis (Herd)
CACHE_DRIVER=redis
SESSION_DRIVER=redis
QUEUE_CONNECTION=redis
REDIS_HOST=127.0.0.1
REDIS_PORT=6379

# Postmark
MAIL_MAILER="postmark"
POSTMARK_TOKEN="your-real-postmark-server-token"
POSTMARK_MESSAGE_STREAM_ID="outbound"
MAIL_FROM_ADDRESS="noreply@agent-ai.test"
MAIL_FROM_NAME="Agent AI"

# Inbound webhook (Postmark → Basic Auth)
AGENT_MAIL="<hash>@inbound.postmarkapp.com"
WEBHOOK_USER="postmark"
WEBHOOK_PASS="your-very-long-random-password-here"

# --------------------------
# LLM — Routing & Roles
# --------------------------
## What is routing?
##  - CLASSIFY: quick, cheap intent detection.
##  - GROUNDED: answers with retrieved facts (pgvector) if hits are good.
##  - SYNTH: larger model for reasoning/synthesis if grounding is weak or input is long.
## How it works:
##  1) We embed the user query and run vector KNN search over emails/attachments/memories.
##  2) If the hit-rate ≥ LLM_GROUNDING_HIT_MIN → GROUNDED; else → SYNTH.
##  3) If tokens_in ≥ LLM_SYNTH_COMPLEXITY_TOKENS, force SYNTH.
LLM_TIMEOUT_MS=120000           # Max request time (ms). Larger = more tolerant to slow models.
LLM_RETRY_MAX=1                 # Retries on 408/429/5xx.
LLM_ROUTING_MODE=auto           # auto | single (single disables routing and uses LLM_PROVIDER/LLM_MODEL)
LLM_GROUNDING_HIT_MIN=0.35      # 0–1.0. Raise if you only want very strong retrieval.
LLM_SYNTH_COMPLEXITY_TOKENS=1200# If input ≥ this estimated token count, go SYNTH.
LLM_MAX_AGENT_STEPS=10          # Safety cap for internal multi-step delegations.

# Role bindings (local-first)
LLM_CLASSIFY_PROVIDER=ollama    # Local-first: fast classifier model
LLM_CLASSIFY_MODEL="mistral-small3.2:24b"  # Example local tag
LLM_CLASSIFY_TOOLS=true
LLM_CLASSIFY_REASONING=false

LLM_GROUNDED_PROVIDER=ollama
LLM_GROUNDED_MODEL="gpt-oss:20b"
LLM_GROUNDED_TOOLS=true
LLM_GROUNDED_REASONING=false

LLM_SYNTH_PROVIDER=ollama
LLM_SYNTH_MODEL="gpt-oss:120b"
LLM_SYNTH_TOOLS=true
LLM_SYNTH_REASONING=true

## Embeddings (pgvector)
##  - Used for grounding (retrieval). Choose a model and set matching DIM.
##  - Examples: mxbai-embed-large → 1024, nomic-embed-text → 768
EMBEDDINGS_PROVIDER=ollama
EMBEDDINGS_MODEL="mxbai-embed-large"
EMBEDDINGS_DIM=1024
EMBEDDINGS_DISTANCE=cosine      # cosine | l2 | ip (must match pgvector ops used by indexes)
EMBEDDINGS_INDEX_LISTS=100      # ivfflat lists (increase for larger datasets)

## Providers
##  - Ollama: local models; pull tags you configure above.
##  - OpenAI/Anthropic: set API keys to route roles to cloud.
OLLAMA_BASE_URL="http://ollama:11434"

## Optional Cloud (leave empty if local-only)
##  - Leave these empty to run fully local via Ollama.
##  - To route any role to cloud, set PROVIDER=openai/anthropic and model accordingly.
OPENAI_API_KEY=
OPENAI_BASE_URL="https://api.openai.com/v1"
ANTHROPIC_API_KEY=
ANTHROPIC_BASE_URL="https://api.anthropic.com"

# ClamAV (container)
CLAMAV_HOST=clamav
CLAMAV_PORT=3310

ATTACH_MAX_SIZE_MB=25
ATTACH_TOTAL_MAX_SIZE_MB=40
FILESYSTEM_DISK=local


## Tuning & Troubleshooting
## Tuning Playbook:
##  - If answers hallucinate → lower LLM_GROUNDING_HIT_MIN or improve the embeddings model.
##  - If everything routes to SYNTH → decrease LLM_SYNTH_COMPLEXITY_TOKENS or improve retrieval (k↑).
##  - If latency too high → pick a smaller GROUNDED model or reduce k; consider disabling reasoning for GROUNDED.
## Troubleshooting:
##  - Vector dim mismatch: check EMBEDDINGS_DIM vs actual model; re-run migrations/backfill.
##  - Missing model tags: change role provider/model or pull tags in Ollama.
##  - No matches in retrieval: verify embeddings present; run embeddings:backfill; inspect stopwords/cleanup.
