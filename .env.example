# ==============================================================================
# Agent-AI — Example Configuration (copy to .env for generic setups)
# ==============================================================================

APP_NAME="Agent AI"
APP_ENV=local
APP_DEBUG=true
APP_URL="http://localhost"
APP_TIMEZONE="Europe/Amsterdam"

# PostgreSQL
DB_CONNECTION=pgsql
DB_HOST=127.0.0.1
DB_PORT=5432
DB_DATABASE=agent_ai
DB_USERNAME=postgres
DB_PASSWORD=

# Redis
CACHE_DRIVER=redis
SESSION_DRIVER=redis
QUEUE_CONNECTION=redis
REDIS_HOST=127.0.0.1
REDIS_PORT=6379

# Postmark
MAIL_MAILER="postmark"
POSTMARK_TOKEN=
POSTMARK_MESSAGE_STREAM_ID="outbound"
MAIL_FROM_ADDRESS="noreply@example.com"
MAIL_FROM_NAME="Agent AI"

# Inbound webhook (Postmark → Basic Auth)
AGENT_MAIL="<hash>@inbound.postmarkapp.com"
WEBHOOK_USER="postmark"
WEBHOOK_PASS="set-a-long-random-password"

# --------------------------
# LLM Routing & Roles
# --------------------------
# Routing picks the right model automatically:
#  - CLASSIFY: detects intent/complexity quickly.
#  - GROUNDED: answers using retrieved facts from your data.
#  - SYNTH: uses a larger model for complex, long, or multi-step tasks.
LLM_TIMEOUT_MS=120000
LLM_RETRY_MAX=1
LLM_ROUTING_MODE=auto       # auto | single (single disables routing)
LLM_GROUNDING_HIT_MIN=0.35  # 0.0–1.0 (how strong retrieval must be to count as a “hit”)
LLM_SYNTH_COMPLEXITY_TOKENS=1200 # ≥ this → SYNTH
LLM_MAX_AGENT_STEPS=10

# Role bindings (default to local Ollama)
LLM_CLASSIFY_PROVIDER=ollama
LLM_CLASSIFY_MODEL="mistral-small3.2:24b"  # example local tag for classifier
LLM_CLASSIFY_TOOLS=true
LLM_CLASSIFY_REASONING=false

LLM_GROUNDED_PROVIDER=ollama
LLM_GROUNDED_MODEL="gpt-oss:20b"
LLM_GROUNDED_TOOLS=true
LLM_GROUNDED_REASONING=false

LLM_SYNTH_PROVIDER=ollama
LLM_SYNTH_MODEL="gpt-oss:120b"
LLM_SYNTH_TOOLS=true
LLM_SYNTH_REASONING=true

# Embeddings (pgvector)
# Choose one Ollama embedding model and set matching DIM:
#   - mxbai-embed-large  (quality) → 1024
#   - nomic-embed-text   (fast)    → 768
#   - embeddinggemma / bge-m3 → set the correct DIM for your tag
EMBEDDINGS_PROVIDER=ollama
EMBEDDINGS_MODEL="mxbai-embed-large"
EMBEDDINGS_DIM=1024
EMBEDDINGS_DISTANCE=cosine  # cosine | l2 | ip (must match index/ops)
EMBEDDINGS_INDEX_LISTS=100

# Providers
#  - To run fully local, install Ollama and pull the models you reference.
#  - To use cloud, set the provider/model for a role and its API key below.
OLLAMA_BASE_URL="http://localhost:11434"

# Optional Cloud (flip any role to these later)
OPENAI_API_KEY=
OPENAI_BASE_URL="https://api.openai.com/v1"
ANTHROPIC_API_KEY=
ANTHROPIC_BASE_URL="https://api.anthropic.com"

# Security / AV
CLAMAV_HOST=127.0.0.1
CLAMAV_PORT=3310

ATTACH_MAX_SIZE_MB=25
ATTACH_TOTAL_MAX_SIZE_MB=40
FILESYSTEM_DISK=local

# --------------------------
# Notes:
# - CLASSIFY → GROUNDED → SYNTH routing is controlled by the env keys above.
# - To use cloud models, set provider/model per role, e.g.:
#     LLM_SYNTH_PROVIDER=openai
#     LLM_SYNTH_MODEL=gpt-5
# - Ensure EMBEDDINGS_DIM matches your embedding model’s vector length.
# --------------------------
